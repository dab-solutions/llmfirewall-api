# Logging configuration
# Set log level for the application (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Performance configuration
# Number of worker threads for processing scan requests (default: 4)
# Adjust based on your server's CPU cores and expected load
THREAD_POOL_WORKERS=4

# Hugging Face API configuration (REQUIRED)
# Get your token from: https://huggingface.co/settings/tokens
# Make sure you have access to: https://huggingface.co/meta-llama/Llama-Prompt-Guard-2-86M
HF_TOKEN=your_huggingface_token_here

# Together API configuration (Optional)
# Required only if using PII_DETECTION scanner
# Get your API key from: https://www.together.ai/
TOGETHER_API_KEY=your_together_api_key_here

# OpenAI API configuration (required if using MODERATION scanner)
# Get your key from: https://platform.openai.com/api-keys
# Your account must be funded to be able to use the moderation endpoint
OPENAI_API_KEY=your_openai_api_key_here

# LlamaFirewall Scanner configuration
# Available scanners: PROMPT_GUARD, PII_DETECTION, HIDDEN_ASCII, AGENT_ALIGNMENT, CODE_SHIELD, MODERATION
# Example configurations:
# - Basic: {"USER": ["PROMPT_GUARD"]}
# - With PII detection: {"USER": ["PROMPT_GUARD", "PII_DETECTION"]}
# - With OpenAI moderation: {"USER": ["PROMPT_GUARD", "MODERATION"]}
# - Full protection: {"USER": ["PROMPT_GUARD", "MODERATION", "PII_DETECTION"]}
LLAMAFIREWALL_SCANNERS={"USER": ["PROMPT_GUARD"]}

# LLM Guard Configuration
# Enable/disable LLM Guard (true/false)
LLMGUARD_ENABLED=false

# LLM Guard Input Scanners (for scanning user prompts)
# Available input scanners: Anonymize, BanCode, BanCompetitors, BanSubstrings, BanTopics, Code, 
# Gibberish, InvisibleText, Language, PromptInjection, Regex, Secrets, Sentiment, TokenLimit, Toxicity
# Example configurations:
# - Basic protection: ["PromptInjection", "Toxicity"]
# - Advanced protection: ["PromptInjection", "Toxicity", "Secrets", "BanCode", "TokenLimit"]
# - Full protection: ["PromptInjection", "Toxicity", "Secrets", "BanCode", "TokenLimit", "Gibberish", "InvisibleText"]
LLMGUARD_INPUT_SCANNERS=["PromptInjection", "Toxicity", "Secrets"]

# LLM Guard Output Scanners (for scanning LLM responses - currently not implemented in this API)
# Available output scanners: BanCode, BanCompetitors, BanSubstrings, BanTopics, Bias, Code, 
# Deanonymize, JSON, Language, LanguageSame, MaliciousURLs, NoRefusal, ReadingTime, 
# FactualConsistency, Gibberish, Regex, Relevance, Sensitive, Sentiment, Toxicity, URLReachability
# Example: ["Bias", "Toxicity", "MaliciousURLs", "Sensitive"]
LLMGUARD_OUTPUT_SCANNERS=[]

# LLM Guard Scanner Thresholds (0.0 to 1.0)
# Higher values = more strict filtering
LLMGUARD_TOXICITY_THRESHOLD=0.7
LLMGUARD_PROMPT_INJECTION_THRESHOLD=0.8
LLMGUARD_SENTIMENT_THRESHOLD=0.5
LLMGUARD_BIAS_THRESHOLD=0.7

# LLM Guard Scanner-specific configurations
# Anonymization settings (if using Anonymize scanner)
# Supported entities: PERSON, EMAIL_ADDRESS, PHONE_NUMBER, CREDIT_CARD, etc.
LLMGUARD_ANONYMIZE_ENTITIES=["PERSON", "EMAIL_ADDRESS", "PHONE_NUMBER"]

# Code languages to scan for (if using Code scanner)
LLMGUARD_CODE_LANGUAGES=["python", "javascript", "java", "sql"]

# Competitors to ban mentions of (if using BanCompetitors scanner)
LLMGUARD_COMPETITORS=["openai", "anthropic", "google"]

# Substrings to ban (if using BanSubstrings scanner)
LLMGUARD_BANNED_SUBSTRINGS=["hack", "exploit", "bypass"]

# Topics to ban (if using BanTopics scanner)
LLMGUARD_BANNED_TOPICS=["violence", "illegal"]

# Valid languages for content (if using Language scanner)
LLMGUARD_VALID_LANGUAGES=["en"]

# Regex patterns for validation (if using Regex scanner)
LLMGUARD_REGEX_PATTERNS=["^(?!.*password).*$"]

# LLM Guard Token Limit (if using TokenLimit scanner)
LLMGUARD_TOKEN_LIMIT=4096

# LLM Guard fail_fast setting (stop on first failure)
LLMGUARD_FAIL_FAST=true

# Tokenizer configuration (Optional)
# Set to false to disable tokenizer parallelism
# This can help with memory usage in some environments
TOKENIZERS_PARALLELISM=false

# Database Configuration
CONFIG_DB_PATH=config.db

# Azure AI Content Safety Configuration (Optional)
# Microsoft's AI service for content moderation and safety
# Get your endpoint and key from: https://portal.azure.com/
# Documentation: https://learn.microsoft.com/en-us/azure/ai-services/content-safety/

# Enable/disable Azure AI Content Safety (true/false)
AZURE_CONTENT_SAFETY_ENABLED=false

# Azure AI Content Safety endpoint URL
# Example: https://your-resource.cognitiveservices.azure.com/
AZURE_CONTENT_SAFETY_ENDPOINT=your_azure_endpoint_here

# Azure AI Content Safety API key
AZURE_CONTENT_SAFETY_KEY=your_azure_api_key_here

# Enable/disable text content analysis (true/false)
AZURE_CONTENT_SAFETY_TEXT_ENABLED=true

# Enable/disable jailbreak detection (true/false)
# Note: Jailbreak detection may not be available in all SDK versions
AZURE_CONTENT_SAFETY_JAILBREAK_ENABLED=false

# Azure Content Safety Severity Thresholds (0-7)
# Content is flagged when severity > threshold
# 0 = most strict (flag almost everything)
# 7 = least strict (flag only the most harmful content)
# Recommended starting values: 2-4 for balanced protection

# Hate speech threshold (0-7)
AZURE_CONTENT_SAFETY_HATE_THRESHOLD=2

# Self-harm content threshold (0-7)
AZURE_CONTENT_SAFETY_SELFHARM_THRESHOLD=2

# Sexual content threshold (0-7)
AZURE_CONTENT_SAFETY_SEXUAL_THRESHOLD=2

# Violence content threshold (0-7)
AZURE_CONTENT_SAFETY_VIOLENCE_THRESHOLD=2

# Jailbreak attempt threshold (0.0-1.0)
# Note: This feature may not be available in current SDK versions
AZURE_CONTENT_SAFETY_JAILBREAK_THRESHOLD=0.5

# Python configuration (Optional)
# These are set by default in the Dockerfile
# PYTHONDONTWRITEBYTECODE=1
# PYTHONUNBUFFERED=1
# PYTHONPATH=/app