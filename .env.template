# Logging configuration
# Set log level for the application (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Performance configuration
# Number of worker threads for processing scan requests (default: 4)
# Adjust based on your server's CPU cores and expected load
THREAD_POOL_WORKERS=4

# Hugging Face API configuration (REQUIRED)
# Get your token from: https://huggingface.co/settings/tokens
# Make sure you have access to: https://huggingface.co/meta-llama/Llama-Prompt-Guard-2-86M
HF_TOKEN=your_huggingface_token_here

# Together API configuration (Optional)
# Required only if using PII_DETECTION scanner
# Get your API key from: https://www.together.ai/
TOGETHER_API_KEY=your_together_api_key_here

# OpenAI API configuration (required if using MODERATION scanner)
# Get your key from: https://platform.openai.com/api-keys
# Your account must be funded to be able to use the moderation endpoint
OPENAI_API_KEY=your_openai_api_key_here

# LlamaFirewall Scanner configuration
# Available scanners: PROMPT_GUARD, PII_DETECTION, HIDDEN_ASCII, AGENT_ALIGNMENT, CODE_SHIELD, MODERATION
# Example configurations:
# - Basic: {"USER": ["PROMPT_GUARD"]}
# - With PII detection: {"USER": ["PROMPT_GUARD", "PII_DETECTION"]}
# - With OpenAI moderation: {"USER": ["PROMPT_GUARD", "MODERATION"]}
# - Full protection: {"USER": ["PROMPT_GUARD", "MODERATION", "PII_DETECTION"]}
LLAMAFIREWALL_SCANNERS={"USER": ["PROMPT_GUARD"]}

# LLM Guard Configuration
# Enable/disable LLM Guard (true/false)
LLMGUARD_ENABLED=false

# LLM Guard Input Scanners (for scanning user prompts)
# Available input scanners: Anonymize, BanCode, BanCompetitors, BanSubstrings, BanTopics, Code, 
# Gibberish, InvisibleText, Language, PromptInjection, Regex, Secrets, Sentiment, TokenLimit, Toxicity
# Example configurations:
# - Basic protection: ["PromptInjection", "Toxicity"]
# - Advanced protection: ["PromptInjection", "Toxicity", "Secrets", "BanCode", "TokenLimit"]
# - Full protection: ["PromptInjection", "Toxicity", "Secrets", "BanCode", "TokenLimit", "Gibberish", "InvisibleText"]
LLMGUARD_INPUT_SCANNERS=["PromptInjection", "Toxicity", "Secrets"]

# LLM Guard Output Scanners (for scanning LLM responses - currently not implemented in this API)
# Available output scanners: BanCode, BanCompetitors, BanSubstrings, BanTopics, Bias, Code, 
# Deanonymize, JSON, Language, LanguageSame, MaliciousURLs, NoRefusal, ReadingTime, 
# FactualConsistency, Gibberish, Regex, Relevance, Sensitive, Sentiment, Toxicity, URLReachability
# Example: ["Bias", "Toxicity", "MaliciousURLs", "Sensitive"]
LLMGUARD_OUTPUT_SCANNERS=[]

# LLM Guard Scanner Thresholds (0.0 to 1.0)
# Higher values = more strict filtering
LLMGUARD_TOXICITY_THRESHOLD=0.7
LLMGUARD_PROMPT_INJECTION_THRESHOLD=0.8
LLMGUARD_SENTIMENT_THRESHOLD=0.5
LLMGUARD_BIAS_THRESHOLD=0.7

# LLM Guard Scanner-specific configurations
# Anonymization settings (if using Anonymize scanner)
# Supported entities: PERSON, EMAIL_ADDRESS, PHONE_NUMBER, CREDIT_CARD, etc.
LLMGUARD_ANONYMIZE_ENTITIES=["PERSON", "EMAIL_ADDRESS", "PHONE_NUMBER"]

# Code languages to scan for (if using Code scanner)
LLMGUARD_CODE_LANGUAGES=["python", "javascript", "java", "sql"]

# Competitors to ban mentions of (if using BanCompetitors scanner)
LLMGUARD_COMPETITORS=["openai", "anthropic", "google"]

# Substrings to ban (if using BanSubstrings scanner)
LLMGUARD_BANNED_SUBSTRINGS=["hack", "exploit", "bypass"]

# Topics to ban (if using BanTopics scanner)
LLMGUARD_BANNED_TOPICS=["violence", "illegal"]

# Valid languages for content (if using Language scanner)
LLMGUARD_VALID_LANGUAGES=["en"]

# Regex patterns for validation (if using Regex scanner)
LLMGUARD_REGEX_PATTERNS=["^(?!.*password).*$"]

# LLM Guard Token Limit (if using TokenLimit scanner)
LLMGUARD_TOKEN_LIMIT=4096

# LLM Guard fail_fast setting (stop on first failure)
LLMGUARD_FAIL_FAST=true

# Tokenizer configuration (Optional)
# Set to false to disable tokenizer parallelism
# This can help with memory usage in some environments
TOKENIZERS_PARALLELISM=false

# Database Configuration
CONFIG_DB_PATH=config.db

# Python configuration (Optional)
# These are set by default in the Dockerfile
# PYTHONDONTWRITEBYTECODE=1
# PYTHONUNBUFFERED=1
# PYTHONPATH=/app